{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet_21 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jySLe0bX-0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "7f1eb8d7-1a35-4ddb-8611-a23da62c2739"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMgLYQbaYDMV",
        "colab_type": "code",
        "outputId": "877fc1ce-7641-45ad-e25a-017632471fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "print(model_type)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet20v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5h_AuNhYlWC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "71d2ec2a-ba06-43c2-fc00-433a79bacbb0"
      },
      "source": [
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu9oB_aUZ8Qx",
        "colab_type": "code",
        "outputId": "80038db8-efa2-4eab-e682-936aac1f74f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUXNl2thaEiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "#    lr = round(0.002 * 1/(1 + 0.15 * epoch), 10)\n",
        "#    print('Learning rate: ', lr)\n",
        "#    return lr\n",
        "    lr = 1e-3\n",
        "    if epoch < 5:\n",
        "        lr = 0.002\n",
        "    elif epoch < 10:\n",
        "        lr = 0.001\n",
        "    elif epoch < 20:\n",
        "        lr = 0.0005\n",
        "    elif epoch < 30:\n",
        "        lr = 0.0001\n",
        "    elif epoch < 35:\n",
        "        lr = 0.000075\n",
        "    elif epoch < 40:\n",
        "        lr = 0.00005\n",
        "    elif epoch < 45:\n",
        "        lr = 0.000025\n",
        "    else:\n",
        "        lr = 0.00001 \n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjajQ4MVbOCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1wmd9hJbbxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='glorot_uniform')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIkwIoRLbgp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7Bz-xQbsOP",
        "colab_type": "code",
        "outputId": "1d1e6e4e-91a0-492c-f585-51908dd0dbfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.002\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 32, 32, 16)   448         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 32, 32, 16)   64          conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 32, 32, 16)   0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 32, 32, 16)   2320        activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 32, 32, 16)   64          conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 32, 32, 16)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 32, 32, 16)   2320        activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 32, 32, 16)   64          conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 32, 32, 16)   0           activation_96[0][0]              \n",
            "                                                                 batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 32, 32, 16)   0           add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 32, 32, 16)   2320        activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 32, 32, 16)   64          conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 32, 32, 16)   0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 32, 32, 16)   2320        activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 32, 32, 16)   64          conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 32, 32, 16)   0           activation_98[0][0]              \n",
            "                                                                 batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 32, 32, 16)   0           add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 32, 32, 16)   2320        activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 32, 32, 16)   64          conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 32, 32, 16)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 32, 32, 16)   2320        activation_101[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 32, 32, 16)   64          conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 32, 32, 16)   0           activation_100[0][0]             \n",
            "                                                                 batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 32, 32, 16)   0           add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 16, 16, 32)   4640        activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 16, 16, 32)   128         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 16, 16, 32)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 16, 16, 32)   9248        activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 16, 16, 32)   544         activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 16, 16, 32)   128         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 16, 16, 32)   0           conv2d_115[0][0]                 \n",
            "                                                                 batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 16, 16, 32)   0           add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 16, 16, 32)   9248        activation_104[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 16, 16, 32)   128         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 16, 16, 32)   0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 16, 16, 32)   9248        activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 16, 16, 32)   128         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 16, 16, 32)   0           activation_104[0][0]             \n",
            "                                                                 batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 16, 16, 32)   0           add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 16, 16, 32)   9248        activation_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 16, 16, 32)   128         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 16, 16, 32)   0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 16, 16, 32)   9248        activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 16, 16, 32)   128         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 16, 16, 32)   0           activation_106[0][0]             \n",
            "                                                                 batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 16, 16, 32)   0           add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 8, 8, 64)     18496       activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 8, 8, 64)     256         conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 8, 8, 64)     0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 8, 8, 64)     36928       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 8, 8, 64)     2112        activation_108[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 8, 8, 64)     256         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 8, 8, 64)     0           conv2d_122[0][0]                 \n",
            "                                                                 batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 8, 8, 64)     0           add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 8, 8, 64)     36928       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 8, 8, 64)     256         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 8, 8, 64)     0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 8, 8, 64)     36928       activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 8, 8, 64)     256         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 8, 8, 64)     0           activation_110[0][0]             \n",
            "                                                                 batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 8, 8, 64)     0           add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 8, 8, 64)     36928       activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 8, 8, 64)     256         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 8, 8, 64)     0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 8, 8, 64)     36928       activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 8, 8, 64)     256         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 8, 8, 64)     0           activation_112[0][0]             \n",
            "                                                                 batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 8, 8, 64)     0           add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 1, 1, 64)     0           activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 64)           0           average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10)           650         flatten_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sV8Et7Fbw66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkPRjZ5kjUcg",
        "colab_type": "code",
        "outputId": "1de55da1-ae7e-4886-d927-eef46a06fd71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=15,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.1,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=get_random_eraser(v_l=0, v_h=1),\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=2, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/50\n",
            "Learning rate:  0.002\n",
            " - 75s - loss: 1.7452 - acc: 0.4178 - val_loss: 1.9535 - val_acc: 0.4356\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.43560, saving model to /content/saved_models/cifar10_ResNet20v1_model.001.h5\n",
            "Epoch 2/50\n",
            "Learning rate:  0.002\n",
            " - 66s - loss: 1.3879 - acc: 0.5620 - val_loss: 1.8820 - val_acc: 0.5122\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.43560 to 0.51220, saving model to /content/saved_models/cifar10_ResNet20v1_model.002.h5\n",
            "Epoch 3/50\n",
            "Learning rate:  0.002\n",
            " - 66s - loss: 1.2410 - acc: 0.6275 - val_loss: 1.4514 - val_acc: 0.5874\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.51220 to 0.58740, saving model to /content/saved_models/cifar10_ResNet20v1_model.003.h5\n",
            "Epoch 4/50\n",
            "Learning rate:  0.002\n",
            " - 66s - loss: 1.1540 - acc: 0.6675 - val_loss: 1.3755 - val_acc: 0.6356\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.58740 to 0.63560, saving model to /content/saved_models/cifar10_ResNet20v1_model.004.h5\n",
            "Epoch 5/50\n",
            "Learning rate:  0.002\n",
            " - 62s - loss: 1.1036 - acc: 0.6882 - val_loss: 1.1808 - val_acc: 0.6901\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.63560 to 0.69010, saving model to /content/saved_models/cifar10_ResNet20v1_model.005.h5\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            " - 63s - loss: 0.9698 - acc: 0.7322 - val_loss: 0.8667 - val_acc: 0.7689\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.69010 to 0.76890, saving model to /content/saved_models/cifar10_ResNet20v1_model.006.h5\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            " - 62s - loss: 0.9152 - acc: 0.7447 - val_loss: 0.8534 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.76890 to 0.77400, saving model to /content/saved_models/cifar10_ResNet20v1_model.007.h5\n",
            "Epoch 8/50\n",
            "Learning rate:  0.001\n",
            " - 62s - loss: 0.8857 - acc: 0.7548 - val_loss: 0.9988 - val_acc: 0.7372\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.77400\n",
            "Epoch 9/50\n",
            "Learning rate:  0.001\n",
            " - 63s - loss: 0.8630 - acc: 0.7621 - val_loss: 0.8999 - val_acc: 0.7518\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.77400\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            " - 63s - loss: 0.8392 - acc: 0.7717 - val_loss: 0.7772 - val_acc: 0.7912\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.77400 to 0.79120, saving model to /content/saved_models/cifar10_ResNet20v1_model.010.h5\n",
            "Epoch 11/50\n",
            "Learning rate:  0.0005\n",
            " - 63s - loss: 0.7638 - acc: 0.7942 - val_loss: 0.6728 - val_acc: 0.8296\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.79120 to 0.82960, saving model to /content/saved_models/cifar10_ResNet20v1_model.011.h5\n",
            "Epoch 12/50\n",
            "Learning rate:  0.0005\n",
            " - 63s - loss: 0.7400 - acc: 0.8003 - val_loss: 0.7200 - val_acc: 0.8138\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.82960\n",
            "Epoch 13/50\n",
            "Learning rate:  0.0005\n",
            " - 62s - loss: 0.7163 - acc: 0.8067 - val_loss: 0.6606 - val_acc: 0.8293\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.82960\n",
            "Epoch 14/50\n",
            "Learning rate:  0.0005\n",
            " - 63s - loss: 0.7030 - acc: 0.8099 - val_loss: 0.7353 - val_acc: 0.8104\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.82960\n",
            "Epoch 15/50\n",
            "Learning rate:  0.0005\n",
            " - 63s - loss: 0.6947 - acc: 0.8126 - val_loss: 0.7641 - val_acc: 0.8083\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.82960\n",
            "Epoch 16/50\n",
            "Learning rate:  0.0005\n",
            " - 65s - loss: 0.6841 - acc: 0.8172 - val_loss: 0.6318 - val_acc: 0.8406\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.82960 to 0.84060, saving model to /content/saved_models/cifar10_ResNet20v1_model.016.h5\n",
            "Epoch 17/50\n",
            "Learning rate:  0.0005\n",
            " - 64s - loss: 0.6730 - acc: 0.8158 - val_loss: 0.6839 - val_acc: 0.8266\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.84060\n",
            "Epoch 18/50\n",
            "Learning rate:  0.0005\n",
            " - 63s - loss: 0.6687 - acc: 0.8210 - val_loss: 0.7024 - val_acc: 0.8197\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.84060\n",
            "Epoch 19/50\n",
            "Learning rate:  0.0005\n",
            " - 63s - loss: 0.6629 - acc: 0.8198 - val_loss: 0.6305 - val_acc: 0.8353\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.84060\n",
            "Epoch 20/50\n",
            "Learning rate:  0.0005\n",
            " - 62s - loss: 0.6492 - acc: 0.8257 - val_loss: 0.7779 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.84060\n",
            "Epoch 21/50\n",
            "Learning rate:  0.0001\n",
            " - 63s - loss: 0.5906 - acc: 0.8465 - val_loss: 0.5636 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.84060 to 0.86060, saving model to /content/saved_models/cifar10_ResNet20v1_model.021.h5\n",
            "Epoch 22/50\n",
            "Learning rate:  0.0001\n",
            " - 63s - loss: 0.5759 - acc: 0.8508 - val_loss: 0.5722 - val_acc: 0.8572\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.86060\n",
            "Epoch 23/50\n",
            "Learning rate:  0.0001\n",
            " - 62s - loss: 0.5623 - acc: 0.8540 - val_loss: 0.6044 - val_acc: 0.8486\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.86060\n",
            "Epoch 24/50\n",
            "Learning rate:  0.0001\n",
            " - 62s - loss: 0.5525 - acc: 0.8573 - val_loss: 0.5298 - val_acc: 0.8707\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.86060 to 0.87070, saving model to /content/saved_models/cifar10_ResNet20v1_model.024.h5\n",
            "Epoch 25/50\n",
            "Learning rate:  0.0001\n",
            " - 62s - loss: 0.5431 - acc: 0.8582 - val_loss: 0.5431 - val_acc: 0.8663\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.87070\n",
            "Epoch 26/50\n",
            "Learning rate:  0.0001\n",
            " - 63s - loss: 0.5437 - acc: 0.8583 - val_loss: 0.5191 - val_acc: 0.8696\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.87070\n",
            "Epoch 27/50\n",
            "Learning rate:  0.0001\n",
            " - 63s - loss: 0.5348 - acc: 0.8607 - val_loss: 0.5302 - val_acc: 0.8674\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.87070\n",
            "Epoch 28/50\n",
            "Learning rate:  0.0001\n",
            " - 62s - loss: 0.5351 - acc: 0.8587 - val_loss: 0.5616 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.87070\n",
            "Epoch 29/50\n",
            "Learning rate:  0.0001\n",
            " - 61s - loss: 0.5337 - acc: 0.8627 - val_loss: 0.5180 - val_acc: 0.8688\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.87070\n",
            "Epoch 30/50\n",
            "Learning rate:  0.0001\n",
            " - 61s - loss: 0.5250 - acc: 0.8634 - val_loss: 0.5154 - val_acc: 0.8719\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.87070 to 0.87190, saving model to /content/saved_models/cifar10_ResNet20v1_model.030.h5\n",
            "Epoch 31/50\n",
            "Learning rate:  7.5e-05\n",
            " - 62s - loss: 0.5168 - acc: 0.8661 - val_loss: 0.5078 - val_acc: 0.8719\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.87190\n",
            "Epoch 32/50\n",
            "Learning rate:  7.5e-05\n",
            " - 65s - loss: 0.5125 - acc: 0.8675 - val_loss: 0.5006 - val_acc: 0.8736\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.87190 to 0.87360, saving model to /content/saved_models/cifar10_ResNet20v1_model.032.h5\n",
            "Epoch 33/50\n",
            "Learning rate:  7.5e-05\n",
            " - 64s - loss: 0.5106 - acc: 0.8672 - val_loss: 0.4996 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.87360 to 0.87620, saving model to /content/saved_models/cifar10_ResNet20v1_model.033.h5\n",
            "Epoch 34/50\n",
            "Learning rate:  7.5e-05\n",
            " - 65s - loss: 0.5047 - acc: 0.8693 - val_loss: 0.5132 - val_acc: 0.8719\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.87620\n",
            "Epoch 35/50\n",
            "Learning rate:  7.5e-05\n",
            " - 63s - loss: 0.4961 - acc: 0.8722 - val_loss: 0.5239 - val_acc: 0.8690\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.87620\n",
            "Epoch 36/50\n",
            "Learning rate:  5e-05\n",
            " - 63s - loss: 0.4979 - acc: 0.8698 - val_loss: 0.4861 - val_acc: 0.8805\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.87620 to 0.88050, saving model to /content/saved_models/cifar10_ResNet20v1_model.036.h5\n",
            "Epoch 37/50\n",
            "Learning rate:  5e-05\n",
            " - 62s - loss: 0.4923 - acc: 0.8727 - val_loss: 0.5324 - val_acc: 0.8674\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.88050\n",
            "Epoch 38/50\n",
            "Learning rate:  5e-05\n",
            " - 62s - loss: 0.4908 - acc: 0.8736 - val_loss: 0.4930 - val_acc: 0.8749\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.88050\n",
            "Epoch 39/50\n",
            "Learning rate:  5e-05\n",
            " - 62s - loss: 0.4865 - acc: 0.8738 - val_loss: 0.5280 - val_acc: 0.8694\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.88050\n",
            "Epoch 40/50\n",
            "Learning rate:  5e-05\n",
            " - 62s - loss: 0.4838 - acc: 0.8767 - val_loss: 0.4917 - val_acc: 0.8764\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.88050\n",
            "Epoch 41/50\n",
            "Learning rate:  2.5e-05\n",
            " - 63s - loss: 0.4777 - acc: 0.8763 - val_loss: 0.4861 - val_acc: 0.8788\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.88050\n",
            "Epoch 42/50\n",
            "Learning rate:  2.5e-05\n",
            " - 61s - loss: 0.4789 - acc: 0.8763 - val_loss: 0.4853 - val_acc: 0.8783\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.88050\n",
            "Epoch 43/50\n",
            "Learning rate:  2.5e-05\n",
            " - 60s - loss: 0.4766 - acc: 0.8785 - val_loss: 0.5053 - val_acc: 0.8746\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.88050\n",
            "Epoch 44/50\n",
            "Learning rate:  2.5e-05\n",
            " - 60s - loss: 0.4769 - acc: 0.8770 - val_loss: 0.4953 - val_acc: 0.8776\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.88050\n",
            "Epoch 45/50\n",
            "Learning rate:  2.5e-05\n",
            " - 61s - loss: 0.4815 - acc: 0.8750 - val_loss: 0.4896 - val_acc: 0.8767\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.88050\n",
            "Epoch 46/50\n",
            "Learning rate:  1e-05\n",
            " - 61s - loss: 0.4666 - acc: 0.8795 - val_loss: 0.4898 - val_acc: 0.8788\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.88050\n",
            "Epoch 47/50\n",
            "Learning rate:  1e-05\n",
            " - 61s - loss: 0.4646 - acc: 0.8816 - val_loss: 0.4887 - val_acc: 0.8787\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.88050\n",
            "Epoch 48/50\n",
            "Learning rate:  1e-05\n",
            " - 60s - loss: 0.4703 - acc: 0.8794 - val_loss: 0.4956 - val_acc: 0.8755\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.88050\n",
            "Epoch 49/50\n",
            "Learning rate:  1e-05\n",
            " - 61s - loss: 0.4691 - acc: 0.8792 - val_loss: 0.4905 - val_acc: 0.8770\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.88050\n",
            "Epoch 50/50\n",
            "Learning rate:  1e-05\n",
            " - 60s - loss: 0.4645 - acc: 0.8787 - val_loss: 0.4881 - val_acc: 0.8776\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.88050\n",
            "10000/10000 [==============================] - 2s 237us/step\n",
            "Test loss: 0.48810769333839416\n",
            "Test accuracy: 0.8776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyeS9vjvjilx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "fb4f1722-802a-4284-f3cc-f5bc581a9a78"
      },
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()\n",
        "# plot model history\n",
        "plot_model_history(model_info)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOnAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLcsk_8yvzTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}