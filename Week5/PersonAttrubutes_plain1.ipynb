{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersonAttrubutes_plain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "67e3637b-cd24-43be-cd13-8846891c747d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# p mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data_224.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "ls: cannot access 'drive': Transport endpoint is not connected\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "b106a47d-aac6-4707-dc91-1a2998008947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "d737f876-34b2-481f-fd1e-c09374cf88b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        " # load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "c4cd6183-e715-46c3-e534-eef734624939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation=augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"])/255 for _, item in items.iterrows()])\n",
        "        if self.augmentation is not None:\n",
        "          image = self.augmentation.flow(image, shuffle=False).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "       # print(\"A\")\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "        #    print(\"B\")\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "4b5ebea6-c2c8-48db-b65e-694e97a09366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=1)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "20b9aad9-637b-4476-997b-cee7e2e83f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>resized/59.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2106</th>\n",
              "      <td>resized/2107.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5206</th>\n",
              "      <td>resized/5207.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1163</th>\n",
              "      <td>resized/1164.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13534</th>\n",
              "      <td>resized/13536.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "58        resized/59.jpg              0  ...                        1              0\n",
              "2106    resized/2107.jpg              1  ...                        1              0\n",
              "5206    resized/5207.jpg              0  ...                        1              0\n",
              "1163    resized/1164.jpg              1  ...                        1              0\n",
              "13534  resized/13536.jpg              1  ...                        0              1\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## create train and validation data generators\n",
        "import skimage.exposure\n",
        "def AHE(img):\n",
        "    img_adapteq = skimage.exposure.equalize_adapthist(img, clip_limit=0.03)\n",
        "    return img_adapteq\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32, augmentation=ImageDataGenerator(horizontal_flip=True, vertical_flip=True, preprocessing_function=AHE))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, shuffle=False, augmentation=ImageDataGenerator(preprocessing_function=AHE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "b02ba166-fb5a-4943-df99-0167f6a818d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:135: UserWarning: Possible precision loss when converting from float32 to uint16\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:135: UserWarning: Possible precision loss when converting from float64 to float32\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nDE2AHaz38h",
        "colab_type": "code",
        "outputId": "f59aaf88-cfd2-42b1-b7f6-ceb97ac40108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Convolution2D, Flatten, MaxPooling2D, AveragePooling2D, Activation, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "\n",
        "def convx(input_img, ch, ker1, ker2, type):\n",
        "  x = Conv2D(ch, (ker1,ker2), padding=type, use_bias=False)(input_img)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  return x \n",
        "\n",
        "input_img = Input(shape=(224,224,3))\n",
        "x = convx(input_img, 32, 3, 3, 'same')\n",
        "x = convx(x, 32, 3, 3, 'same')\n",
        "x = MaxPooling2D((2,2), strides=2)(x)\n",
        "x = Dropout(0.1)(x)\n",
        "\n",
        "x = convx(x, 64, 3, 3, 'same')\n",
        "x = convx(x, 64, 3, 3, 'same')\n",
        "x = MaxPooling2D((2,2), strides=2)(x)\n",
        "x = Dropout(0.15)(x)\n",
        "\n",
        "x = convx(x, 128, 3, 3, 'same')\n",
        "x = convx(x, 128, 3, 3, 'same')\n",
        "x = MaxPooling2D((2,2), strides=2)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = convx(x, 256, 3, 3, 'same')\n",
        "x = convx(x, 256, 3, 3, 'same')\n",
        "x = MaxPooling2D((2,2), strides=2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = convx(x, 512, 3, 3, 'same')\n",
        "x = convx(x, 512, 3, 3, 'same')\n",
        "x = MaxPooling2D((2,2), strides=2)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = AveragePooling2D(7)(x)\n",
        "\n",
        "model2 = Model(inputs=input_img, outputs=x)\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 224, 224, 32)      864       \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 224, 224, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 224, 224, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 224, 224, 32)      9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 224, 224, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 224, 224, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 112, 112, 64)      18432     \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 112, 112, 64)      36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 112, 112, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 56, 56, 128)       73728     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 56, 56, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 56, 56, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 28, 28, 256)       294912    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 28, 28, 256)       589824    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 14, 14, 512)       1179648   \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 14, 14, 512)       2359296   \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 14, 14, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 1, 1, 512)         0         \n",
            "=================================================================\n",
            "Total params: 4,718,176\n",
            "Trainable params: 4,714,208\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#backbone = VGG16(\n",
        "#    weights=\"imagenet\", \n",
        "#    include_top=False, \n",
        "#    input_tensor=Input(shape=(224, 224, 3))\n",
        "#)\n",
        "backbone = model2\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\", kernel_initializer='glorot_uniform'\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.optimizers import Adam\n",
        "losses = {\n",
        " \t\"gender_output\": \"binary_crossentropy\",\n",
        " \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        " \t\"age_output\": \"mse\",\n",
        " \t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        " }\n",
        "loss_weights = {\"gender_output\": 1.5, \"image_quality_output\": 1.5, \"age_output\": 2, \"weight_output\": 1.2, \"bag_output\": 1.2, \"footwear_output\": 1.5, \"pose_output\": 1.0, \"emotion_output\": 1.0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaTaMgvKK12Q",
        "colab_type": "code",
        "outputId": "2c51eaa6-7cb6-4d47-a27a-a930ef590524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import *\n",
        "filepath=\"/content/gdrive/My Drive/MyCNN/epochs:_1.hdf5\"\n",
        "def scheduler(epoch, lr):\n",
        "  print('Learning rate: ', lr)\n",
        "  return lr\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "lr_reducer = ReduceLROnPlateau(monitor='loss',\n",
        "                               factor=0.5,\n",
        "                               cooldown=0,\n",
        "                               patience=15,\n",
        "                               min_lr=0.5e-6)\n",
        "csv_logger = CSVLogger('/content/gdrive/My Drive/MyCNN/_log.csv', append=True, separator=',')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [lr_scheduler, lr_reducer]\n",
        "\n",
        "opt = SGD(lr=0.1, momentum=0.9, nesterov=True, decay=0.1)\n",
        "with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        model.compile(\n",
        "        optimizer=opt,\n",
        "        loss= losses, \n",
        "        loss_weights=loss_weights, \n",
        "        metrics=[\"accuracy\"]\n",
        "        )\n",
        "        model_info = model.fit_generator(\n",
        "        generator=train_gen,\n",
        "        validation_data=valid_gen,\n",
        "        use_multiprocessing=True,\n",
        "        callbacks=callbacks_list,\n",
        "        initial_epoch=0,\n",
        "        workers=6, \n",
        "        epochs=300,\n",
        "        verbose=1 \n",
        "        )\n",
        "\n",
        "#model.load_weights('/content/gdrive/My Drive/MyCNN/epochs: 1.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 273s 758ms/step - loss: 10.4454 - gender_output_loss: 0.7676 - image_quality_output_loss: 1.2424 - age_output_loss: 0.1485 - weight_output_loss: 1.3090 - bag_output_loss: 1.2878 - footwear_output_loss: 1.3108 - pose_output_loss: 1.0220 - emotion_output_loss: 1.0289 - gender_output_acc: 0.5556 - image_quality_output_acc: 0.5440 - age_output_acc: 0.4023 - weight_output_acc: 0.6193 - bag_output_acc: 0.5520 - footwear_output_acc: 0.5077 - pose_output_acc: 0.6129 - emotion_output_acc: 0.7117 - val_loss: 8.4925 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1487 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.9407 - val_footwear_output_loss: 0.9923 - val_pose_output_loss: 0.9262 - val_emotion_output_loss: 0.9776 - val_gender_output_acc: 0.5645 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6394 - val_bag_output_acc: 0.5481 - val_footwear_output_acc: 0.5179 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 2/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3530 - gender_output_loss: 0.6832 - image_quality_output_loss: 0.9793 - age_output_loss: 0.1457 - weight_output_loss: 0.9813 - bag_output_loss: 0.9131 - footwear_output_loss: 0.9813 - pose_output_loss: 0.9251 - emotion_output_loss: 0.8975 - gender_output_acc: 0.5654 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5280 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7160 - val_loss: 8.4714 - val_gender_output_loss: 0.6831 - val_image_quality_output_loss: 0.9813 - val_age_output_loss: 0.1481 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.9365 - val_footwear_output_loss: 0.9940 - val_pose_output_loss: 0.9264 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5640 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5481 - val_footwear_output_acc: 0.5139 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 3/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 267s 742ms/step - loss: 8.3451 - gender_output_loss: 0.6831 - image_quality_output_loss: 0.9787 - age_output_loss: 0.1456 - weight_output_loss: 0.9803 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9798 - pose_output_loss: 0.9244 - emotion_output_loss: 0.8959 - gender_output_acc: 0.5691 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5310 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 8.4932 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1479 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.9369 - val_footwear_output_loss: 1.0086 - val_pose_output_loss: 0.9265 - val_emotion_output_loss: 0.9602 - val_gender_output_acc: 0.5640 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3666 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5030 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 4/300\n",
            "Learning rate:  0.10000000149011612\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3397 - gender_output_loss: 0.6824 - image_quality_output_loss: 0.9786 - age_output_loss: 0.1455 - weight_output_loss: 0.9804 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9785 - pose_output_loss: 0.9246 - emotion_output_loss: 0.8936 - gender_output_acc: 0.5689 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4046 - weight_output_acc: 0.6347 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5356 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7167Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3410 - gender_output_loss: 0.6825 - image_quality_output_loss: 0.9784 - age_output_loss: 0.1455 - weight_output_loss: 0.9807 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9786 - pose_output_loss: 0.9243 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5689 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6344 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5353 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 8.4802 - val_gender_output_loss: 0.6831 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1478 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.9368 - val_footwear_output_loss: 1.0010 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9604 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3666 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5104 - val_pose_output_acc: 0.6141 - val_emotion_output_acc: 0.6855\n",
            "Epoch 5/300\n",
            "Learning rate:  0.10000000149011612\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3345 - gender_output_loss: 0.6823 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9801 - bag_output_loss: 0.9122 - footwear_output_loss: 0.9750 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8965 - gender_output_acc: 0.5673 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4040 - weight_output_acc: 0.6343 - bag_output_acc: 0.5662 - footwear_output_acc: 0.5354 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7155Epoch 5/300\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3336 - gender_output_loss: 0.6823 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9800 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9750 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8958 - gender_output_acc: 0.5672 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.5661 - footwear_output_acc: 0.5354 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7160 - val_loss: 8.4751 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1478 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.9363 - val_footwear_output_loss: 0.9996 - val_pose_output_loss: 0.9264 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5124 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 6/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3361 - gender_output_loss: 0.6820 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1455 - weight_output_loss: 0.9800 - bag_output_loss: 0.9122 - footwear_output_loss: 0.9763 - pose_output_loss: 0.9242 - emotion_output_loss: 0.8955 - gender_output_acc: 0.5687 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4037 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5345 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 8.4815 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1477 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9363 - val_footwear_output_loss: 1.0033 - val_pose_output_loss: 0.9265 - val_emotion_output_loss: 0.9593 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 7/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 268s 746ms/step - loss: 8.3361 - gender_output_loss: 0.6817 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1454 - weight_output_loss: 0.9803 - bag_output_loss: 0.9120 - footwear_output_loss: 0.9772 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8952 - gender_output_acc: 0.5682 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5366 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4761 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1477 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9365 - val_footwear_output_loss: 0.9996 - val_pose_output_loss: 0.9264 - val_emotion_output_loss: 0.9595 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5104 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 8/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3293 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1454 - weight_output_loss: 0.9791 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9745 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8954 - gender_output_acc: 0.5673 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4040 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5347 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7163 - val_loss: 8.4796 - val_gender_output_loss: 0.6830 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1477 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.9366 - val_footwear_output_loss: 1.0019 - val_pose_output_loss: 0.9264 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5481 - val_footwear_output_acc: 0.5089 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 9/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 269s 747ms/step - loss: 8.3311 - gender_output_loss: 0.6821 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1454 - weight_output_loss: 0.9793 - bag_output_loss: 0.9121 - footwear_output_loss: 0.9745 - pose_output_loss: 0.9241 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5647 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4044 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5345 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7166 - val_loss: 8.4845 - val_gender_output_loss: 0.6832 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 0.1477 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.9367 - val_footwear_output_loss: 1.0044 - val_pose_output_loss: 0.9265 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5040 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 10/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 269s 746ms/step - loss: 8.3314 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9783 - age_output_loss: 0.1455 - weight_output_loss: 0.9796 - bag_output_loss: 0.9119 - footwear_output_loss: 0.9747 - pose_output_loss: 0.9241 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5697 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4044 - weight_output_acc: 0.6346 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5371 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7159 - val_loss: 8.4716 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1477 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9974 - val_pose_output_loss: 0.9264 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5119 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 11/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 269s 747ms/step - loss: 8.3312 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1454 - weight_output_loss: 0.9797 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9753 - pose_output_loss: 0.9242 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5670 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4045 - weight_output_acc: 0.6346 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5352 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7161 - val_loss: 8.4873 - val_gender_output_loss: 0.6830 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1477 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9372 - val_footwear_output_loss: 1.0062 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5000 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 12/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 270s 749ms/step - loss: 8.3253 - gender_output_loss: 0.6821 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1454 - weight_output_loss: 0.9793 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9719 - pose_output_loss: 0.9233 - emotion_output_loss: 0.8946 - gender_output_acc: 0.5702 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6350 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5387 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7161 - val_loss: 8.4813 - val_gender_output_loss: 0.6831 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 0.1477 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9369 - val_footwear_output_loss: 1.0021 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9594 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 13/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 269s 748ms/step - loss: 8.3307 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1454 - weight_output_loss: 0.9797 - bag_output_loss: 0.9120 - footwear_output_loss: 0.9746 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8953 - gender_output_acc: 0.5694 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5340 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7160 - val_loss: 8.4741 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1477 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9367 - val_footwear_output_loss: 0.9984 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5119 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 14/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 269s 746ms/step - loss: 8.3266 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1454 - weight_output_loss: 0.9790 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9735 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5695 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4040 - weight_output_acc: 0.6349 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5359 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7163 - val_loss: 8.4792 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9367 - val_footwear_output_loss: 1.0013 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 15/300\n",
            "Learning rate:  0.10000000149011612\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "360/360 [==============================] - 270s 750ms/step - loss: 8.3302 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1454 - weight_output_loss: 0.9791 - bag_output_loss: 0.9121 - footwear_output_loss: 0.9747 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8953 - gender_output_acc: 0.5682 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5349 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4724 - val_gender_output_loss: 0.6830 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9366 - val_footwear_output_loss: 0.9972 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5630 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5124 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 16/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 740ms/step - loss: 8.3248 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1454 - weight_output_loss: 0.9787 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9720 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5691 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5382 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4681 - val_gender_output_loss: 0.6830 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9365 - val_footwear_output_loss: 0.9949 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5129 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 17/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 740ms/step - loss: 8.3287 - gender_output_loss: 0.6820 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1454 - weight_output_loss: 0.9790 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9734 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5669 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4040 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5373 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4751 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9367 - val_footwear_output_loss: 0.9992 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 18/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3258 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1454 - weight_output_loss: 0.9782 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9731 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8950 - gender_output_acc: 0.5694 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4042 - weight_output_acc: 0.6348 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5376 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.4784 - val_gender_output_loss: 0.6827 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.9368 - val_footwear_output_loss: 1.0009 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 19/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3264 - gender_output_loss: 0.6818 - image_quality_output_loss: 0.9771 - age_output_loss: 0.1453 - weight_output_loss: 0.9793 - bag_output_loss: 0.9118 - footwear_output_loss: 0.9726 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8953 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5535 - age_output_acc: 0.4044 - weight_output_acc: 0.6346 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5387 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4757 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9367 - val_footwear_output_loss: 0.9993 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5481 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 20/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 268s 745ms/step - loss: 8.3243 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9786 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9728 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5685 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5393 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4792 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9821 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9367 - val_footwear_output_loss: 1.0013 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5481 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 21/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 267s 741ms/step - loss: 8.3256 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1454 - weight_output_loss: 0.9791 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9728 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5680 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5415 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 8.4741 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9366 - val_footwear_output_loss: 0.9985 - val_pose_output_loss: 0.9268 - val_emotion_output_loss: 0.9593 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 22/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 268s 744ms/step - loss: 8.3237 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9724 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5683 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4044 - weight_output_acc: 0.6348 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5379 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4684 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.9363 - val_footwear_output_loss: 0.9953 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5099 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 23/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 740ms/step - loss: 8.3279 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1454 - weight_output_loss: 0.9796 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9733 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5662 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5662 - footwear_output_acc: 0.5366 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7158 - val_loss: 8.4760 - val_gender_output_loss: 0.6830 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9366 - val_footwear_output_loss: 0.9995 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5035 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 24/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3256 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9789 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9726 - pose_output_loss: 0.9242 - emotion_output_loss: 0.8946 - gender_output_acc: 0.5667 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5368 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7162 - val_loss: 8.4715 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9973 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9588 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 25/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3230 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1454 - weight_output_loss: 0.9791 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8949 - gender_output_acc: 0.5706 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4037 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5395 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4704 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9966 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 26/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 265s 735ms/step - loss: 8.3224 - gender_output_loss: 0.6811 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1453 - weight_output_loss: 0.9782 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9719 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5707 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4042 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5392 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7162 - val_loss: 8.4694 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1475 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9960 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5084 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 27/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 264s 734ms/step - loss: 8.3278 - gender_output_loss: 0.6820 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9784 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9736 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8953 - gender_output_acc: 0.5675 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4041 - weight_output_acc: 0.6346 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5386 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7160 - val_loss: 8.4701 - val_gender_output_loss: 0.6830 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9959 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5094 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 28/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3265 - gender_output_loss: 0.6817 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1454 - weight_output_loss: 0.9793 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9728 - pose_output_loss: 0.9228 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4042 - weight_output_acc: 0.6344 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5381 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7160 - val_loss: 8.4692 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9362 - val_footwear_output_loss: 0.9957 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9588 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5089 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 29/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3269 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1454 - weight_output_loss: 0.9793 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9726 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8953 - gender_output_acc: 0.5687 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4040 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5385 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7159 - val_loss: 8.4716 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9789 - val_bag_output_loss: 0.9363 - val_footwear_output_loss: 0.9968 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5084 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 30/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3267 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1453 - weight_output_loss: 0.9792 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9732 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8952 - gender_output_acc: 0.5701 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4042 - weight_output_acc: 0.6344 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5373 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7160 - val_loss: 8.4690 - val_gender_output_loss: 0.6827 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9363 - val_footwear_output_loss: 0.9956 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5099 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 31/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3254 - gender_output_loss: 0.6817 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1454 - weight_output_loss: 0.9788 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9718 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8957 - gender_output_acc: 0.5659 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5398 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7158 - val_loss: 8.4710 - val_gender_output_loss: 0.6827 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9363 - val_footwear_output_loss: 0.9968 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 32/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 265s 735ms/step - loss: 8.3229 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1454 - weight_output_loss: 0.9784 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9727 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5691 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5388 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4705 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9963 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9595 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5069 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "\n",
            "Learning rate: Epoch 33/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3225 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1453 - weight_output_loss: 0.9792 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8943 - gender_output_acc: 0.5693 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5662 - footwear_output_acc: 0.5412 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 8.4727 - val_gender_output_loss: 0.6827 - val_image_quality_output_loss: 0.9820 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9365 - val_footwear_output_loss: 0.9974 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9595 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 34/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 264s 733ms/step - loss: 8.3192 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9694 - pose_output_loss: 0.9233 - emotion_output_loss: 0.8946 - gender_output_acc: 0.5676 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4038 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5401 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7161 - val_loss: 8.4713 - val_gender_output_loss: 0.6828 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9968 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5089 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 35/300\n",
            "Learning rate:  0.10000000149011612\n",
            "Epoch 34/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 263s 729ms/step - loss: 8.3265 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1453 - weight_output_loss: 0.9791 - bag_output_loss: 0.9107 - footwear_output_loss: 0.9735 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8949 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5386 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4716 - val_gender_output_loss: 0.6827 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9971 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 36/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 262s 729ms/step - loss: 8.3208 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1453 - weight_output_loss: 0.9794 - bag_output_loss: 0.9107 - footwear_output_loss: 0.9698 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8952 - gender_output_acc: 0.5712 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5405 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4653 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9362 - val_footwear_output_loss: 0.9938 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5099 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 37/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3216 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9783 - age_output_loss: 0.1453 - weight_output_loss: 0.9786 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9702 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5698 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4040 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5427 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7161 - val_loss: 8.4667 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9362 - val_footwear_output_loss: 0.9945 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5089 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 38/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3236 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9783 - age_output_loss: 0.1452 - weight_output_loss: 0.9791 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9717 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5717 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4037 - weight_output_acc: 0.6346 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5367 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4708 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9968 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5030 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 39/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3255 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9790 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9730 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4044 - weight_output_acc: 0.6348 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5384 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.4697 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9959 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5040 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 40/300\n",
            "Learning rate:  0.10000000149011612\n",
            "0.10000000149011612\n",
            "360/360 [==============================] - 264s 734ms/step - loss: 8.3215 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1454 - weight_output_loss: 0.9784 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9712 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5734 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5380 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4727 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9365 - val_footwear_output_loss: 0.9977 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5035 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 41/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 263s 731ms/step - loss: 8.3240 - gender_output_loss: 0.6819 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1453 - weight_output_loss: 0.9785 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9711 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8952 - gender_output_acc: 0.5644 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4044 - weight_output_acc: 0.6346 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5430 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4647 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9936 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5084 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 42/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 267s 742ms/step - loss: 8.3253 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1454 - weight_output_loss: 0.9786 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9728 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5677 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4038 - weight_output_acc: 0.6349 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5404 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4709 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9363 - val_footwear_output_loss: 0.9966 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 43/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 268s 744ms/step - loss: 8.3230 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1453 - weight_output_loss: 0.9783 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9722 - pose_output_loss: 0.9233 - emotion_output_loss: 0.8941 - gender_output_acc: 0.5694 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4040 - weight_output_acc: 0.6348 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5405 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7162 - val_loss: 8.4669 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.9362 - val_footwear_output_loss: 0.9945 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 44/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 269s 748ms/step - loss: 8.3215 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1454 - weight_output_loss: 0.9784 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9713 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5674 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4040 - weight_output_acc: 0.6349 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5396 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4695 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.9364 - val_footwear_output_loss: 0.9959 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 45/300\n",
            "Learning rate:  0.10000000149011612\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3228 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1453 - weight_output_loss: 0.9790 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9709 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5669 - image_quality_output_acc: 0.5526 - age_output_acc: 0.4045 - weight_output_acc: 0.6345 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5414 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7161Epoch 45/300\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3213 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9786 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9709 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8943 - gender_output_acc: 0.5668 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4042 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5413 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 8.4669 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9362 - val_footwear_output_loss: 0.9944 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9593 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 46/300\n",
            "Learning rate:  0.10000000149011612\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3206 - gender_output_loss: 0.6808 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9778 - bag_output_loss: 0.9107 - footwear_output_loss: 0.9723 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8937 - gender_output_acc: 0.5698 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4046 - weight_output_acc: 0.6349 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5398 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7165\n",
            "360/360 [==============================] - 268s 744ms/step - loss: 8.3228 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1453 - weight_output_loss: 0.9781 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9726 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5694 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4044 - weight_output_acc: 0.6348 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5397 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7164 - val_loss: 8.4690 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9818 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9362 - val_footwear_output_loss: 0.9958 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 47/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3205 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1452 - weight_output_loss: 0.9786 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9712 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8938 - gender_output_acc: 0.5670 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4041 - weight_output_acc: 0.6347 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5403 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 8.4637 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9930 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 48/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 266s 740ms/step - loss: 8.3237 - gender_output_loss: 0.6820 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9715 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5668 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5408 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7163 - val_loss: 8.4673 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9949 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 49/300\n",
            "Learning rate:  0.10000000149011612\n",
            "360/360 [==============================] - 268s 744ms/step - loss: 8.3241 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1454 - weight_output_loss: 0.9784 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9718 - pose_output_loss: 0.9230 - emotion_output_loss: 0.8950 - gender_output_acc: 0.5677 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5662 - footwear_output_acc: 0.5417 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7160 - val_loss: 8.4687 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9957 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 50/300\n",
            "Learning rate:  0.05000000074505806\n",
            "Epoch 49/300\n",
            "360/360 [==============================] - 267s 741ms/step - loss: 8.3212 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1454 - weight_output_loss: 0.9782 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9709 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5702 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5379 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4683 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9956 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 51/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 269s 748ms/step - loss: 8.3194 - gender_output_loss: 0.6818 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9779 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9701 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5673 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5411 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7163 - val_loss: 8.4635 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9931 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 52/300\n",
            "Learning rate:  0.05000000074505806\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3215 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1452 - weight_output_loss: 0.9775 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9713 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5702 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4039 - weight_output_acc: 0.6349 - bag_output_acc: 0.5669 - footwear_output_acc: 0.5397 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7163Epoch 52/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 269s 747ms/step - loss: 8.3227 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1452 - weight_output_loss: 0.9780 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9714 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5701 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4036 - weight_output_acc: 0.6347 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5398 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7164 - val_loss: 8.4664 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9946 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5030 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 53/300\n",
            "Learning rate:  0.05000000074505806\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3222 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1453 - weight_output_loss: 0.9783 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9726 - pose_output_loss: 0.9232 - emotion_output_loss: 0.8941 - gender_output_acc: 0.5708 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4044 - weight_output_acc: 0.6351 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5405 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7161Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 740ms/step - loss: 8.3232 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1453 - weight_output_loss: 0.9789 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9726 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8937 - gender_output_acc: 0.5706 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5405 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7163 - val_loss: 8.4640 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9933 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5035 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 54/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3229 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9783 - age_output_loss: 0.1454 - weight_output_loss: 0.9784 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9723 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5727 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4038 - weight_output_acc: 0.6344 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5386 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4644 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9934 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 55/300\n",
            "Learning rate:  0.05000000074505806\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 54/300\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3207 - gender_output_loss: 0.6818 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9795 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9695 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5691 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4042 - weight_output_acc: 0.6347 - bag_output_acc: 0.5669 - footwear_output_acc: 0.5424 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7164 - val_loss: 8.4675 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9953 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5035 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 56/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 740ms/step - loss: 8.3200 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9784 - age_output_loss: 0.1453 - weight_output_loss: 0.9785 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9695 - pose_output_loss: 0.9230 - emotion_output_loss: 0.8950 - gender_output_acc: 0.5699 - image_quality_output_acc: 0.5525 - age_output_acc: 0.4040 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5411 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4617 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9921 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5094 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 57/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3218 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1452 - weight_output_loss: 0.9781 - bag_output_loss: 0.9118 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9243 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5714 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4045 - weight_output_acc: 0.6346 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5400 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7161 - val_loss: 8.4644 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9937 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 57/300\n",
            "Learning rate:  0.05000000074505806\n",
            "Epoch 58/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3224 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1453 - weight_output_loss: 0.9783 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9718 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5650 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5392 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7163 - val_loss: 8.4672 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9951 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5040 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 59/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3201 - gender_output_loss: 0.6819 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1452 - weight_output_loss: 0.9780 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9702 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5675 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4044 - weight_output_acc: 0.6349 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5418 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 8.4617 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9922 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5069 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 60/300\n",
            "Learning rate:  0.05000000074505806\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3236 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9772 - age_output_loss: 0.1452 - weight_output_loss: 0.9788 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9724 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5684 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4042 - weight_output_acc: 0.6344 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5397 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7158Epoch 60/300\n",
            "360/360 [==============================] - 263s 730ms/step - loss: 8.3242 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1453 - weight_output_loss: 0.9789 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9727 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4040 - weight_output_acc: 0.6344 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5393 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4652 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9940 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 61/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 741ms/step - loss: 8.3187 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9784 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9707 - pose_output_loss: 0.9232 - emotion_output_loss: 0.8938 - gender_output_acc: 0.5703 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4047 - weight_output_acc: 0.6346 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5437 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7164 - val_loss: 8.4635 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9931 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5030 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 62/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 742ms/step - loss: 8.3213 - gender_output_loss: 0.6818 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1453 - weight_output_loss: 0.9785 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9705 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5690 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6344 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5395 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7162 - val_loss: 8.4637 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9930 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9588 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 63/300\n",
            "Learning rate:  0.05000000074505806\n",
            "\n",
            "360/360 [==============================] - 268s 745ms/step - loss: 8.3226 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1453 - weight_output_loss: 0.9773 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9730 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5700 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6351 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5420 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.4642 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9932 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 64/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3217 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1454 - weight_output_loss: 0.9790 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5677 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5406 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4628 - val_gender_output_loss: 0.6825 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9926 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9588 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 65/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3167 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9789 - bag_output_loss: 0.9106 - footwear_output_loss: 0.9693 - pose_output_loss: 0.9232 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5697 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5672 - footwear_output_acc: 0.5416 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 8.4654 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9941 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 66/300\n",
            "Learning rate:  0.05000000074505806\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3201 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9771 - age_output_loss: 0.1453 - weight_output_loss: 0.9778 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9723 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5713 - image_quality_output_acc: 0.5535 - age_output_acc: 0.4044 - weight_output_acc: 0.6351 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5404 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7162Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 269s 747ms/step - loss: 8.3229 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9727 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5712 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4044 - weight_output_acc: 0.6345 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5400 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7163 - val_loss: 8.4662 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9945 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 67/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 268s 744ms/step - loss: 8.3225 - gender_output_loss: 0.6821 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9713 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5635 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4050 - weight_output_acc: 0.6346 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5413 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4619 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9923 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9588 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 68/300\n",
            "Learning rate:  0.05000000074505806\n",
            "Epoch 67/300\n",
            "360/360 [==============================] - 269s 747ms/step - loss: 8.3219 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1453 - weight_output_loss: 0.9785 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9708 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5649 - image_quality_output_acc: 0.5524 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5425 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4675 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9953 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5040 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 69/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 269s 747ms/step - loss: 8.3234 - gender_output_loss: 0.6811 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9120 - footwear_output_loss: 0.9715 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5695 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5396 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4637 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9932 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 70/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3186 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1452 - weight_output_loss: 0.9784 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9702 - pose_output_loss: 0.9233 - emotion_output_loss: 0.8938 - gender_output_acc: 0.5700 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4046 - weight_output_acc: 0.6351 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5427 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7166 - val_loss: 8.4628 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9927 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 71/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 268s 745ms/step - loss: 8.3217 - gender_output_loss: 0.6808 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9787 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9711 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8950 - gender_output_acc: 0.5704 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5391 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7163 - val_loss: 8.4669 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9949 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 72/300\n",
            "Learning rate:  0.05000000074505806\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3194 - gender_output_loss: 0.6811 - image_quality_output_loss: 0.9771 - age_output_loss: 0.1453 - weight_output_loss: 0.9786 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8943 - gender_output_acc: 0.5694 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5404 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7161 - val_loss: 8.4656 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9943 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5030 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 73/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 268s 745ms/step - loss: 8.3191 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1452 - weight_output_loss: 0.9777 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5658 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4039 - weight_output_acc: 0.6348 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5419 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 8.4615 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9920 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 74/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 269s 746ms/step - loss: 8.3202 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9773 - age_output_loss: 0.1453 - weight_output_loss: 0.9782 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9714 - pose_output_loss: 0.9241 - emotion_output_loss: 0.8936 - gender_output_acc: 0.5691 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4044 - weight_output_acc: 0.6350 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5399 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7164 - val_loss: 8.4609 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9917 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9588 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5040 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 75/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 741ms/step - loss: 8.3230 - gender_output_loss: 0.6817 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9711 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5687 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4041 - weight_output_acc: 0.6349 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5396 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.4659 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9944 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5040 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 76/300\n",
            "Learning rate:  0.05000000074505806\n",
            "Epoch 75/300\n",
            "360/360 [==============================] - 267s 742ms/step - loss: 8.3190 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1451 - weight_output_loss: 0.9785 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9712 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5710 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6347 - bag_output_acc: 0.5669 - footwear_output_acc: 0.5411 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4633 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9929 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 77/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 742ms/step - loss: 8.3217 - gender_output_loss: 0.6817 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9776 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9713 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8950 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4039 - weight_output_acc: 0.6346 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5418 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7159 - val_loss: 8.4655 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9942 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 78/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 268s 744ms/step - loss: 8.3201 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1453 - weight_output_loss: 0.9784 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9708 - pose_output_loss: 0.9228 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5699 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6346 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5421 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7161 - val_loss: 8.4672 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9951 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5030 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 79/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 742ms/step - loss: 8.3194 - gender_output_loss: 0.6801 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1452 - weight_output_loss: 0.9787 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9703 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5736 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4045 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5394 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7159 - val_loss: 8.4646 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9937 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 80/300\n",
            "Learning rate:  0.05000000074505806\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3237 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9780 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9734 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8943 - gender_output_acc: 0.5668 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4046 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5372 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7161 - val_loss: 8.4632 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9929 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 81/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3213 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1453 - weight_output_loss: 0.9782 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9721 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8941 - gender_output_acc: 0.5694 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5399 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7164 - val_loss: 8.4618 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9921 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 82/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3213 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1453 - weight_output_loss: 0.9790 - bag_output_loss: 0.9118 - footwear_output_loss: 0.9702 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5715 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5662 - footwear_output_acc: 0.5440 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 8.4649 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9938 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 83/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3196 - gender_output_loss: 0.6808 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9778 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5727 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4039 - weight_output_acc: 0.6348 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5424 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7162 - val_loss: 8.4655 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9942 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 84/300\n",
            "Learning rate:  0.02500000037252903\n",
            "Epoch 83/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3192 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1454 - weight_output_loss: 0.9785 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9699 - pose_output_loss: 0.9230 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5701 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4039 - weight_output_acc: 0.6346 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5408 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7164 - val_loss: 8.4649 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9938 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 85/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3210 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9787 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9711 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5673 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4040 - weight_output_acc: 0.6346 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5420 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7162 - val_loss: 8.4650 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9939 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 86/300\n",
            "Learning rate:  0.02500000037252903\n",
            "Epoch 85/300\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3198 - gender_output_loss: 0.6811 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1452 - weight_output_loss: 0.9782 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8937 - gender_output_acc: 0.5687 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5432 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7165 - val_loss: 8.4621 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9924 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 87/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3209 - gender_output_loss: 0.6800 - image_quality_output_loss: 0.9773 - age_output_loss: 0.1453 - weight_output_loss: 0.9787 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9722 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8949 - gender_output_acc: 0.5714 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5411 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7159 - val_loss: 8.4624 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9925 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 88/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3220 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1454 - weight_output_loss: 0.9786 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9719 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8952 - gender_output_acc: 0.5708 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5401 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4615 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9921 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 89/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 265s 735ms/step - loss: 8.3218 - gender_output_loss: 0.6804 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9787 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9722 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5707 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6344 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5413 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.4625 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9925 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 90/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 265s 735ms/step - loss: 8.3222 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9782 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9728 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5742 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4039 - weight_output_acc: 0.6346 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5386 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7163 - val_loss: 8.4656 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9942 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 91/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3211 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9708 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8946 - gender_output_acc: 0.5674 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5418 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 8.4653 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9940 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 92/300\n",
            "Learning rate:  0.02500000037252903\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3216 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1452 - weight_output_loss: 0.9773 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9723 - pose_output_loss: 0.9242 - emotion_output_loss: 0.8932 - gender_output_acc: 0.5672 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6348 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5386 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7164Epoch 92/300\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3213 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1452 - weight_output_loss: 0.9775 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9722 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8935 - gender_output_acc: 0.5672 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4042 - weight_output_acc: 0.6347 - bag_output_acc: 0.5662 - footwear_output_acc: 0.5388 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7163 - val_loss: 8.4613 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9919 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 93/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3219 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1452 - weight_output_loss: 0.9782 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9714 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5716 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4042 - weight_output_acc: 0.6347 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5423 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 8.4615 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9920 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 94/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 264s 733ms/step - loss: 8.3219 - gender_output_loss: 0.6805 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1453 - weight_output_loss: 0.9778 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9719 - pose_output_loss: 0.9238 - emotion_output_loss: 0.8950 - gender_output_acc: 0.5719 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5669 - footwear_output_acc: 0.5420 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7161 - val_loss: 8.4625 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9925 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 95/300\n",
            "Learning rate:  0.02500000037252903\n",
            "360/360 [==============================] - 264s 733ms/step - loss: 8.3174 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9771 - age_output_loss: 0.1453 - weight_output_loss: 0.9782 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9692 - pose_output_loss: 0.9239 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4045 - weight_output_acc: 0.6345 - bag_output_acc: 0.5669 - footwear_output_acc: 0.5425 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 8.4642 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9934 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 96/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 264s 733ms/step - loss: 8.3178 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9773 - age_output_loss: 0.1454 - weight_output_loss: 0.9781 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9699 - pose_output_loss: 0.9229 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4041 - weight_output_acc: 0.6347 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5440 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4630 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9928 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 97/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 264s 733ms/step - loss: 8.3176 - gender_output_loss: 0.6805 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1453 - weight_output_loss: 0.9782 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9700 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5701 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4044 - weight_output_acc: 0.6349 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5387 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7165 - val_loss: 8.4629 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9927 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 98/300\n",
            "Learning rate:  0.012500000186264515\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3233 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1454 - weight_output_loss: 0.9779 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9722 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8941 - gender_output_acc: 0.5682 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4037 - weight_output_acc: 0.6350 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5370 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7165Epoch 98/300\n",
            "360/360 [==============================] - 263s 731ms/step - loss: 8.3226 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1454 - weight_output_loss: 0.9778 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9719 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5684 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4039 - weight_output_acc: 0.6350 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5374 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7163 - val_loss: 8.4625 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9926 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 99/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 263s 730ms/step - loss: 8.3177 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1452 - weight_output_loss: 0.9773 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9700 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5697 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5406 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7161 - val_loss: 8.4630 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9928 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 100/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 262s 727ms/step - loss: 8.3156 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9768 - age_output_loss: 0.1453 - weight_output_loss: 0.9786 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9693 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8936 - gender_output_acc: 0.5700 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4040 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5440 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7164 - val_loss: 8.4671 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9817 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9786 - val_bag_output_loss: 0.9361 - val_footwear_output_loss: 0.9950 - val_pose_output_loss: 0.9267 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5625 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5035 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 101/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 261s 725ms/step - loss: 8.3220 - gender_output_loss: 0.6811 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9784 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9717 - pose_output_loss: 0.9238 - emotion_output_loss: 0.8946 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4044 - weight_output_acc: 0.6346 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5408 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.4600 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9912 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 102/300\n",
            "Learning rate:  0.012500000186264515\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3169 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1453 - weight_output_loss: 0.9765 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9707 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8931 - gender_output_acc: 0.5646 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4046 - weight_output_acc: 0.6356 - bag_output_acc: 0.5669 - footwear_output_acc: 0.5421 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7165Epoch 102/300\n",
            "360/360 [==============================] - 261s 725ms/step - loss: 8.3186 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1453 - weight_output_loss: 0.9779 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9707 - pose_output_loss: 0.9230 - emotion_output_loss: 0.8931 - gender_output_acc: 0.5646 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4042 - weight_output_acc: 0.6350 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5419 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7165 - val_loss: 8.4622 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9924 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 103/300\n",
            "Learning rate:  0.012500000186264515\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3221 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9784 - age_output_loss: 0.1453 - weight_output_loss: 0.9788 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9708 - pose_output_loss: 0.9232 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5675 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4039 - weight_output_acc: 0.6344 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5425 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7162Epoch 103/300\n",
            "360/360 [==============================] - 263s 732ms/step - loss: 8.3224 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9783 - age_output_loss: 0.1453 - weight_output_loss: 0.9787 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9707 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8950 - gender_output_acc: 0.5674 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4038 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5425 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4607 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9916 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 104/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 264s 734ms/step - loss: 8.3217 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9793 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4044 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5419 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7162 - val_loss: 8.4622 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9924 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5069 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 105/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 264s 734ms/step - loss: 8.3204 - gender_output_loss: 0.6805 - image_quality_output_loss: 0.9778 - age_output_loss: 0.1453 - weight_output_loss: 0.9775 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9715 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8946 - gender_output_acc: 0.5692 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4043 - weight_output_acc: 0.6349 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5396 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7161 - val_loss: 8.4617 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9921 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5069 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 106/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 265s 736ms/step - loss: 8.3206 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9772 - age_output_loss: 0.1453 - weight_output_loss: 0.9786 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9714 - pose_output_loss: 0.9233 - emotion_output_loss: 0.8949 - gender_output_acc: 0.5700 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4040 - weight_output_acc: 0.6346 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5375 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7160 - val_loss: 8.4631 - val_gender_output_loss: 0.6822 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9929 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5069 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 107/300\n",
            "Learning rate:  0.012500000186264515\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3200 - gender_output_loss: 0.6808 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1453 - weight_output_loss: 0.9792 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9705 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5722 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4040 - weight_output_acc: 0.6341 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5411 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161Epoch 107/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 264s 732ms/step - loss: 8.3187 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9773 - age_output_loss: 0.1452 - weight_output_loss: 0.9786 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9703 - pose_output_loss: 0.9232 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5720 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5413 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7160 - val_loss: 8.4610 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9918 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 108/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 264s 733ms/step - loss: 8.3203 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1452 - weight_output_loss: 0.9778 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9713 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5671 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4040 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5405 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7161 - val_loss: 8.4624 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9926 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 109/300\n",
            "Learning rate:  0.012500000186264515\n",
            "Epoch 108/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 265s 735ms/step - loss: 8.3199 - gender_output_loss: 0.6808 - image_quality_output_loss: 0.9780 - age_output_loss: 0.1452 - weight_output_loss: 0.9785 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9232 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5700 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5397 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7162 - val_loss: 8.4607 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9917 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 109/300\n",
            "Epoch 110/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3204 - gender_output_loss: 0.6809 - image_quality_output_loss: 0.9772 - age_output_loss: 0.1453 - weight_output_loss: 0.9786 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9711 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8945 - gender_output_acc: 0.5694 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4043 - weight_output_acc: 0.6348 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5419 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7162 - val_loss: 8.4643 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9937 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9588 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 110/300\n",
            "Epoch 111/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 267s 741ms/step - loss: 8.3181 - gender_output_loss: 0.6803 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9777 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6350 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5439 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7162 - val_loss: 8.4650 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9940 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 112/300\n",
            "Learning rate:  0.012500000186264515\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3189 - gender_output_loss: 0.6805 - image_quality_output_loss: 0.9783 - age_output_loss: 0.1453 - weight_output_loss: 0.9772 - bag_output_loss: 0.9106 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9243 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5701 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4045 - weight_output_acc: 0.6347 - bag_output_acc: 0.5671 - footwear_output_acc: 0.5388 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7162Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 267s 741ms/step - loss: 8.3181 - gender_output_loss: 0.6803 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9777 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5729 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6350 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5439 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7162 - val_loss: 8.4650 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9940 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5610 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 112/300\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3194 - gender_output_loss: 0.6804 - image_quality_output_loss: 0.9782 - age_output_loss: 0.1453 - weight_output_loss: 0.9779 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9708 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5703 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6344 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5393 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 8.4647 - val_gender_output_loss: 0.6822 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9938 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9593 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 113/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 264s 733ms/step - loss: 8.3187 - gender_output_loss: 0.6805 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1452 - weight_output_loss: 0.9783 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9705 - pose_output_loss: 0.9231 - emotion_output_loss: 0.8949 - gender_output_acc: 0.5712 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6342 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5395 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7160 - val_loss: 8.4645 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9937 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5055 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 114/300\n",
            "Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 265s 735ms/step - loss: 8.3223 - gender_output_loss: 0.6805 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9792 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9716 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5731 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5385 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7160 - val_loss: 8.4624 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9926 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5079 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 115/300\n",
            "Learning rate:  0.012500000186264515\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3238 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1452 - weight_output_loss: 0.9789 - bag_output_loss: 0.9119 - footwear_output_loss: 0.9707 - pose_output_loss: 0.9244 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5647 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4046 - weight_output_acc: 0.6346 - bag_output_acc: 0.5662 - footwear_output_acc: 0.5418 - pose_output_acc: 0.6176 - emotion_output_acc: 0.7161\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 115/300Learning rate:  0.012500000186264515\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3229 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9777 - age_output_loss: 0.1453 - weight_output_loss: 0.9784 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9709 - pose_output_loss: 0.9240 - emotion_output_loss: 0.8951 - gender_output_acc: 0.5648 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6346 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5413 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7161 - val_loss: 8.4609 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9917 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5069 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 116/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3208 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1453 - weight_output_loss: 0.9782 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9713 - pose_output_loss: 0.9243 - emotion_output_loss: 0.8943 - gender_output_acc: 0.5713 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6346 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5373 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4600 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9913 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 117/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3209 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1452 - weight_output_loss: 0.9785 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9704 - pose_output_loss: 0.9238 - emotion_output_loss: 0.8943 - gender_output_acc: 0.5676 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4046 - weight_output_acc: 0.6348 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5406 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 8.4653 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9941 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 118/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3192 - gender_output_loss: 0.6811 - image_quality_output_loss: 0.9772 - age_output_loss: 0.1453 - weight_output_loss: 0.9779 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9707 - pose_output_loss: 0.9232 - emotion_output_loss: 0.8947 - gender_output_acc: 0.5682 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4041 - weight_output_acc: 0.6348 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5433 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 8.4618 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9922 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 119/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 265s 735ms/step - loss: 8.3203 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9775 - age_output_loss: 0.1454 - weight_output_loss: 0.9779 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9714 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5692 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5411 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7161 - val_loss: 8.4616 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9921 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5069 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 120/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "Epoch 119/300\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3192 - gender_output_loss: 0.6810 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1452 - weight_output_loss: 0.9784 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9700 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8944 - gender_output_acc: 0.5670 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4040 - weight_output_acc: 0.6348 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5415 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4650 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9939 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 121/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "359/360 [============================>.] - ETA: 0s - loss: 8.3228 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9784 - age_output_loss: 0.1453 - weight_output_loss: 0.9781 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9723 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8935 - gender_output_acc: 0.5669 - image_quality_output_acc: 0.5527 - age_output_acc: 0.4040 - weight_output_acc: 0.6343 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5407 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7166Epoch 121/300\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3224 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9783 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9721 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5673 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5407 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 8.4614 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9920 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 122/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 266s 739ms/step - loss: 8.3226 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1452 - weight_output_loss: 0.9785 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9721 - pose_output_loss: 0.9238 - emotion_output_loss: 0.8948 - gender_output_acc: 0.5706 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5400 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7161 - val_loss: 8.4622 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9925 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 123/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 265s 737ms/step - loss: 8.3171 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1453 - weight_output_loss: 0.9778 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9690 - pose_output_loss: 0.9238 - emotion_output_loss: 0.8940 - gender_output_acc: 0.5692 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6349 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5435 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7164 - val_loss: 8.4618 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9922 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5069 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 124/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 267s 740ms/step - loss: 8.3212 - gender_output_loss: 0.6812 - image_quality_output_loss: 0.9781 - age_output_loss: 0.1452 - weight_output_loss: 0.9780 - bag_output_loss: 0.9115 - footwear_output_loss: 0.9705 - pose_output_loss: 0.9233 - emotion_output_loss: 0.8953 - gender_output_acc: 0.5703 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5431 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4657 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9943 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 125/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 266s 740ms/step - loss: 8.3201 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9785 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8937 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5423 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 8.4624 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9926 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 126/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 266s 738ms/step - loss: 8.3197 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9779 - age_output_loss: 0.1452 - weight_output_loss: 0.9784 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9695 - pose_output_loss: 0.9242 - emotion_output_loss: 0.8953 - gender_output_acc: 0.5690 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4043 - weight_output_acc: 0.6345 - bag_output_acc: 0.5671 - footwear_output_acc: 0.5417 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7160 - val_loss: 8.4635 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9932 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 127/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 266s 740ms/step - loss: 8.3201 - gender_output_loss: 0.6816 - image_quality_output_loss: 0.9776 - age_output_loss: 0.1453 - weight_output_loss: 0.9785 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8937 - gender_output_acc: 0.5688 - image_quality_output_acc: 0.5529 - age_output_acc: 0.4041 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5423 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7163 - val_loss: 8.4624 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9926 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5074 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3205 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9772 - age_output_loss: 0.1453 - weight_output_loss: 0.9776 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9714 - pose_output_loss: 0.9237 - emotion_output_loss: 0.8946 - gender_output_acc: 0.5696 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5412 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7160 - val_loss: 8.4614 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9920 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9591 - val_gender_output_acc: 0.5620 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5064 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 128/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 268s 745ms/step - loss: 8.3188 - gender_output_loss: 0.6807 - image_quality_output_loss: 0.9771 - age_output_loss: 0.1452 - weight_output_loss: 0.9793 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9706 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8942 - gender_output_acc: 0.5713 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4044 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5404 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4653 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9941 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5050 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 129/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 267s 742ms/step - loss: 8.3181 - gender_output_loss: 0.6814 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1452 - weight_output_loss: 0.9780 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9700 - pose_output_loss: 0.9234 - emotion_output_loss: 0.8938 - gender_output_acc: 0.5675 - image_quality_output_acc: 0.5531 - age_output_acc: 0.4046 - weight_output_acc: 0.6348 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5402 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7164 - val_loss: 8.4641 - val_gender_output_loss: 0.6824 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9935 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5060 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 130/300\n",
            "Learning rate:  0.0062500000931322575\n",
            "360/360 [==============================] - 268s 744ms/step - loss: 8.3198 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9772 - age_output_loss: 0.1452 - weight_output_loss: 0.9783 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9711 - pose_output_loss: 0.9238 - emotion_output_loss: 0.8939 - gender_output_acc: 0.5706 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4043 - weight_output_acc: 0.6350 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5404 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7161 - val_loss: 8.4638 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9815 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.9933 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9590 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 131/300\n",
            "Learning rate:  0.0031250000465661287\n",
            "360/360 [==============================] - 267s 743ms/step - loss: 8.3198 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9784 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9710 - pose_output_loss: 0.9236 - emotion_output_loss: 0.8950 - gender_output_acc: 0.5705 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4038 - weight_output_acc: 0.6345 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5396 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7159 - val_loss: 8.4609 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9358 - val_footwear_output_loss: 0.9918 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9589 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5045 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 132/300\n",
            "Learning rate:  0.0031250000465661287\n",
            "360/360 [==============================] - 268s 744ms/step - loss: 8.3199 - gender_output_loss: 0.6813 - image_quality_output_loss: 0.9774 - age_output_loss: 0.1453 - weight_output_loss: 0.9781 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9705 - pose_output_loss: 0.9235 - emotion_output_loss: 0.8949 - gender_output_acc: 0.5661 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4042 - weight_output_acc: 0.6345 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5415 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7161 - val_loss: 8.4661 - val_gender_output_loss: 0.6823 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 0.1476 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9945 - val_pose_output_loss: 0.9266 - val_emotion_output_loss: 0.9592 - val_gender_output_acc: 0.5615 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3656 - val_weight_output_acc: 0.6399 - val_bag_output_acc: 0.5486 - val_footwear_output_acc: 0.5035 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.6855\n",
            "Epoch 133/300\n",
            "Learning rate:  0.0031250000465661287\n",
            " 94/360 [======>.......................] - ETA: 2:56 - loss: 8.3221 - gender_output_loss: 0.6801 - image_quality_output_loss: 0.9827 - age_output_loss: 0.1459 - weight_output_loss: 0.9859 - bag_output_loss: 0.9008 - footwear_output_loss: 0.9694 - pose_output_loss: 0.9318 - emotion_output_loss: 0.8861 - gender_output_acc: 0.5751 - image_quality_output_acc: 0.5512 - age_output_acc: 0.4059 - weight_output_acc: 0.6290 - bag_output_acc: 0.5615 - footwear_output_acc: 0.5372 - pose_output_acc: 0.6107 - emotion_output_acc: 0.7191"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUOyV7p2Y2eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}